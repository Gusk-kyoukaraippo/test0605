import os
import shutil
import streamlit as st
import json
import logging
import uuid
import streamlit.components.v1 as components

from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    PromptTemplate,
    Settings,
)
from llama_index.core.llms import ChatMessage
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.embeddings.openai import OpenAIEmbedding

from google.cloud import storage
from google.cloud import logging as google_logging
from google.cloud.logging.handlers import CloudLoggingHandler
from google.oauth2 import service_account

st.set_page_config(page_title="ãƒ•ãƒ©ãƒ³ã‚±ãƒ³AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚¹ãƒˆ(ãƒãƒ£ãƒƒãƒˆç‰ˆ)", layout="wide")
logger = logging.getLogger(__name__)

LOCAL_INDEX_DIR = "downloaded_storage_openai_embed"

# [å¤‰æ›´] ãƒšãƒ«ã‚½ãƒŠè¨­å®šã¨JSONå‡ºåŠ›å½¢å¼ã‚’æŒ‡ç¤ºã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
SYSTEM_PROMPT = """
LLMã¸ã®æŒ‡ç¤ºæ–‡ï¼ˆãƒ•ãƒ©ãƒ³ã‚±ãƒ³æ°ã®æ„è¦‹æ´»ç”¨ãƒ»ä¿®æ­£æ¡ˆï¼‰
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã¨ã€ãã‚Œã«é–¢é€£ã™ã‚‹ã€å‚ç…§æƒ…å ±ã€‘ã‚’å…ƒã«å¯¾è©±ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã¨ãƒ«ãƒ¼ãƒ«ã‚’å³æ ¼ã«å®ˆã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨å¯¾è©±ã—ã¦ãã ã•ã„ã€‚

# ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š
åå‰: ãƒ•ãƒ©ãƒ³ã‚±ãƒ³
å½¹å‰²: å¤œã®è¡—ã®ç‰‡éš…ã§ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã«ç«‹ã¤ã€äººç”ŸçµŒé¨“è±Šå¯Œãªã‚ªãƒã‚¨ã®ãƒãƒã€‚
æ€§æ ¼: æ°—ã•ãã§é¢å€’è¦‹ãŒè‰¯ãã€ã©ã‚“ãªè©±ã«ã‚‚å„ªã—ãè€³ã‚’å‚¾ã‘ã‚‹ã€‚äººç”Ÿã®é…¸ã„ã‚‚ç”˜ã„ã‚‚å™›ã¿åˆ†ã‘ãŸé‹­ã„æ´å¯ŸåŠ›ã‚’æŒã¡ã€æ™‚ã«ç›¸è«‡è€…ã®ç”˜ãˆã‚„è¦‹æ „ã‚’ã€æ„›ã®ã‚ã‚‹å³ã—ã•ã§ãƒ”ã‚·ãƒ£ãƒªã¨æŒ‡æ‘˜ã™ã‚‹ã€‚
å£èª¿:
ä¸€äººç§°ã¯ã€Œã‚¢ã‚¿ã‚·ã€ã€äºŒäººç§°ã¯ã€Œã‚¢ãƒ³ã‚¿ã€ã€‚
ã‚¿ãƒ¡å£ã‚’åŸºæœ¬ã¨ã—ãŸã€è¦ªã—ã¿ã‚„ã™ã„ã‚ªãƒã‚¨å£èª¿ï¼ˆã€Œã€œã‚ˆã€ã€Œã€œã ã‚ã€ã€Œã€œãªã®ã‚ˆã€ã€Œã€œã‹ã—ã‚‰ã€ãªã©ï¼‰ã§è©±ã™ã€‚
æ•¬ä½“ï¼ˆã§ã™ãƒ»ã¾ã™èª¿ï¼‰ã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„ã€‚
# æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ï¼ˆå›ç­”ã‚’è€ƒãˆã‚‹ã¨ãã®æ‰‹é †ï¼‰
å…±æ„Ÿã¨å—ã‘æ­¢ã‚:ã¾ãšã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¨€è‘‰ã‚’åŒ…ã¿è¾¼ã‚€ã‚ˆã†ã«å„ªã—ãå—ã‘æ­¢ã‚ã€åŠ´ã„ã‚„å…±æ„Ÿã®æ°—æŒã¡ã‚’è¡¨ç¾ã™ã‚‹ã€‚

æœ¬è³ªã®å–ç ´:æ¬¡ã«ã€ã‚ãªãŸã«ä¸ãˆã‚‰ã‚ŒãŸ**ã€å‚ç…§æƒ…å ±ã€‘ã‚’æ·±ãèª­ã¿è§£ãã€‚** ã“ã‚Œã«ã¯å®¢è¦³çš„ãªäº‹å®Ÿã«åŠ ãˆã€ã€Œãƒ•ãƒ©ãƒ³ã‚±ãƒ³ã€ã¨ã„ã†äººç‰©ã®æ„è¦‹ãŒå¤šãå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç†è§£ã™ã‚‹ã€‚
ã‚ãªãŸã¯ãã®æ„è¦‹ã‚’åŸå‘³ã—ã€ã‚ãŸã‹ã‚‚ãã‚ŒãŒå…ƒã€…è‡ªåˆ†è‡ªèº«ã®æŒè«–ã§ã‚ã£ãŸã‹ã®ã‚ˆã†ã«ã€å®Œå…¨ã«è‡ªåˆ†ã®è¨€è‘‰ãƒ»è‡ªåˆ†ã®è¦–ç‚¹ã¨ã—ã¦å†æ§‹ç¯‰ã™ã‚‹ã€‚ ãã®é‹­ã„æ´å¯Ÿã‚’æ ¹æ‹ ã«ã€ç›¸è«‡ã®æ ¸å¿ƒã‚’çªãæŒ‡æ‘˜ã‚’è¡Œã†ã€‚

æ„›ã®ã‚ã‚‹ãŠèª¬æ•™ã¨å¿œæ´:ãã®äº‹å®Ÿã‚„æŒè«–ã‚’è¸ã¾ãˆãŸä¸Šã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã«å–ã‚‹ã¹ãè¡Œå‹•ã‚’ã€å…·ä½“çš„ã‹ã¤å°‘ã—å¼·å¼•ãªãã‚‰ã„ãƒãƒƒã‚­ãƒªã¨ç¤ºã™ã€‚æœ€å¾Œã¯å¿œæ´ã®è¨€è‘‰ã§ç· ã‚ããã‚‹ã€‚

# å³å®ˆã™ã¹ããƒ«ãƒ¼ãƒ«
å›ç­”ã¯ã€ã„ã‹ãªã‚‹æ™‚ã‚‚å¿…ãšãƒšãƒ«ã‚½ãƒŠï¼ˆã‚ªãƒã‚¨ã®ãƒãƒãƒ»ãƒ•ãƒ©ãƒ³ã‚±ãƒ³ï¼‰ã¨ã—ã¦è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
ã€æœ€é‡è¦ã€‘ å›ç­”ã®ä¸­ã«ã€Œå‚ç…§æƒ…å ±ã«ã‚ˆã‚‹ã¨ã€ã€Œãƒ•ãƒ©ãƒ³ã‚±ãƒ³æ°ã®æ„è¦‹ã§ã¯ã€ã¨ã„ã£ãŸæƒ…å ±æºã‚’ç¤ºã™è¨€è‘‰ã‚„ã€ã€ŒAIã€ã€Œã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ãªã©èˆå°è£ã‚’åŒ‚ã‚ã›ã‚‹è¨€è‘‰ã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚ã‚ãªãŸã¯ã‚ãã¾ã§ã€å‚ç…§æƒ…å ±ã®å†…å®¹ã‚’å®Œå…¨ã«è‡ªåˆ†ã®ã‚‚ã®ã¨ã—ã¦ã€è‡ªèº«ã®äººç”ŸçµŒé¨“ã‹ã‚‰èªã£ã¦ã„ã‚‹ã¨ã„ã†è¨­å®šã‚’è²«ã„ã¦ãã ã•ã„ã€‚
æœ€çµ‚çš„ãªå›ç­”ã®æ–‡å­—æ•°ã¯ã€å…¨ä½“ã§200å­—ã‹ã‚‰300å­—ç¨‹åº¦ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
# å‡ºåŠ›å½¢å¼
ã‚ãªãŸã®å¿œç­”ã¯ã€å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã«å¾“ã£ã¦ãã ã•ã„ã€‚ä»–ã®å½¢å¼ã¯ä¸€åˆ‡èªã‚ã‚‰ã‚Œã¾ã›ã‚“ã€‚

```json
{
  "response": "ã“ã“ã«ã€ä¸Šè¨˜ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã«å¾“ã£ã¦ç”Ÿæˆã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã®å›ç­”ã‚’è¨˜è¿°ã—ã¾ã™ã€‚",
  "reasoning": "ã“ã“ã«ã¯ã€ãªãœãã®å›ç­”ã‚’ç”Ÿæˆã—ãŸã®ã‹ã€ã‚ãªãŸã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹ç™ºè€…å‘ã‘ã«ç°¡æ½”ã«è¨˜è¿°ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€ã©ã®å‚ç…§æƒ…å ±ã‚’é‡è¦–ã—ãŸã‹ã€ã©ã®ã‚ˆã†ãªè«–ç†ã§çµè«–ã«è‡³ã£ãŸã‹ãªã©ã‚’å«ã‚ã¦ãã ã•ã„ã€‚å‚ç…§æƒ…å ±ã®å¼•ç”¨ã¯ä¸€éƒ¨ã®ã¿ã¨ã—ã¦ãã ã•ã„ã€‚"
}
```
"""

# æ¤œç´¢ã—ãŸæƒ…å ±ã‚’LLMã«æ¸¡ã™éš›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
CONTEXT_PROMPT_TEMPLATE = """
å‚ç…§æƒ…å ±:
---------------------
{context_str}
---------------------
ä¸Šè¨˜ã®å‚ç…§æƒ…å ±ã‚’è¸ã¾ãˆã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆãªã•ã„ã€‚
"""

# ä¼šè©±å±¥æ­´ã‚’å…ƒã«æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å†ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
CONDENSE_QUESTION_PROMPT_TEMPLATE = """
ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã¨æœ€å¾Œã®è³ªå•ãŒä¸ãˆã‚‰ã‚Œã¾ã™ã€‚ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®æ–‡è„ˆã‚’ä½¿ã£ã¦ã€é–¢é€£ã™ã‚‹ä¼šè©±ã‚’ç››ã‚Šè¾¼ã‚“ã ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ã®è³ªå•ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚

ãƒãƒ£ãƒƒãƒˆå±¥æ­´:
---------------------
{chat_history}
---------------------
æœ€å¾Œã®è³ªå•: {question}

ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ã®è³ªå•:
"""

# --- 1. è¨­å®šã¨åˆæœŸåŒ–å‡¦ç† ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "conversation_id" not in st.session_state:
    st.session_state.conversation_id = None
if 'request_id' not in st.session_state:
    st.session_state.request_id = None
if 'feedback_submitted' not in st.session_state:
    st.session_state.feedback_submitted = False
if 'last_response_obj' not in st.session_state:
    st.session_state.last_response_obj = None
if 'scroll_to_bottom' not in st.session_state:
    st.session_state.scroll_to_bottom = False


class RequestIdFormatter(logging.Formatter):
    def format(self, record):
        log_message = super().format(record)
        if hasattr(record, 'json_fields'):
            log_message += f" {record.json_fields}"
        return log_message

@st.cache_resource
def setup_gcp_services():
    st.info("GCPã‚µãƒ¼ãƒ“ã‚¹ã¨ã®æ¥ç¶šã‚’åˆæœŸåŒ–ä¸­...")
    try:
        gcs_service_account_json_str = st.secrets["GCS_SERVICE_ACCOUNT_JSON"]
        parsed_json = json.loads(gcs_service_account_json_str)
        project_id = parsed_json.get("project_id")
        if not project_id:
            raise ValueError("ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã« 'project_id' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        credentials = service_account.Credentials.from_service_account_info(parsed_json)
        st.success(f"GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ '{project_id}' ã®èªè¨¼æƒ…å ±ã‚’æº–å‚™ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        st.error(f"GCS_SERVICE_ACCOUNT_JSON ã®è¨­å®šèª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.stop()

    if logger.hasHandlers():
        logger.handlers.clear()
    
    logger.setLevel(logging.INFO)
    
    sh = logging.StreamHandler()
    sh.setFormatter(RequestIdFormatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(sh)

    try:
        client = google_logging.Client(credentials=credentials, project=project_id)
        handler = CloudLoggingHandler(client, name="franken-ai-prompt-test-chat")
        logger.addHandler(handler)
        logger.info("Google Cloud Loggingã«æ¥ç¶šã—ã¾ã—ãŸã€‚")
    except Exception as e:
        logger.warning(f"Google Cloud Loggingã¨ã®é€£æºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)

    gcs_client = storage.Client(credentials=credentials, project=project_id)
    return gcs_client

@st.cache_resource
def load_llama_index_from_gcs(_gcs_client: storage.Client, bucket_name: str, index_prefix: str):
    if os.path.exists(LOCAL_INDEX_DIR):
        shutil.rmtree(LOCAL_INDEX_DIR)
    os.makedirs(LOCAL_INDEX_DIR, exist_ok=True)
    with st.spinner("åˆå›ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿ä¸­... (ç´„1åˆ†ã‹ã‹ã‚Šã¾ã™)"):
        try:
            bucket = _gcs_client.bucket(bucket_name)
            blobs = list(bucket.list_blobs(prefix=index_prefix))
            if not blobs: return None
            for blob in blobs:
                if blob.name == index_prefix or blob.name.endswith('/'): continue
                relative_path = os.path.relpath(blob.name, index_prefix)
                local_file_path = os.path.join(LOCAL_INDEX_DIR, relative_path)
                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
                blob.download_to_filename(local_file_path)
            Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-large", dimensions=3072)
            storage_context = StorageContext.from_defaults(persist_dir=LOCAL_INDEX_DIR)
            index = load_index_from_storage(storage_context)
            st.success("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            return index
        except Exception as e:
            st.error(f"GCSã‹ã‚‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None

def get_chat_response(index: VectorStoreIndex, query: str, chat_history: list, n_value: int, system_prompt: str, context_prompt_template: str, condense_prompt_template: str):
    try:
        llm = GoogleGenAI(model="gemini-2.5-flash-lite-preview-06-17")
        context_template = PromptTemplate(context_prompt_template)
        condense_template = PromptTemplate(condense_prompt_template)
        
        llama_chat_history = [ChatMessage(role=m["role"], content=m["content"]) for m in chat_history]

        chat_engine = index.as_chat_engine(
            llm=llm,
            similarity_top_k=n_value,
            chat_mode="condense_plus_context",
            system_prompt=system_prompt,
            context_prompt=context_template,
            condense_prompt=condense_template
        )
        with st.spinner('AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­ã§ã™...'):
            response = chat_engine.chat(query, chat_history=llama_chat_history)
        return response
    except Exception as e:
        st.error(f"LLMã‹ã‚‰ã®å¿œç­”å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def main():
    st.title("ğŸ¤– ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚¹ãƒˆ (ãƒãƒ£ãƒƒãƒˆç‰ˆ)")
    st.markdown("ã“ã®ã‚¢ãƒ—ãƒªã¯ã€ãƒ•ãƒ©ãƒ³ã‚±ãƒ³ãƒ©ã‚¸ã‚ªAIã¨ãƒãƒ£ãƒƒãƒˆå½¢å¼ã§ä¼šè©±ãŒã§ãã¾ã™ã€‚")
    st.markdown("---")
    
    try:
        os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
        os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
        GCS_BUCKET_NAME = st.secrets["GCS_BUCKET_NAME"]
        GCS_INDEX_PREFIX = st.secrets["GCS_INDEX_PREFIX"]
    except KeyError as e:
        st.error(f"å¿…è¦ãªè¨­å®šãŒ secrets.toml ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}ã€‚")
        st.stop()

    gcs_client = setup_gcp_services()
    if not gcs_client:
        st.stop()
        
    llama_index = load_llama_index_from_gcs(gcs_client, GCS_BUCKET_NAME, GCS_INDEX_PREFIX)

    if llama_index:
        st.sidebar.header("âš™ï¸ é«˜åº¦ãªè¨­å®š")
        custom_system_prompt = st.sidebar.text_area("ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (ãƒšãƒ«ã‚½ãƒŠè¨­å®š):", SYSTEM_PROMPT, height=400)
        n_value = st.sidebar.slider("é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢æ•° (Nå€¤)", 1, 10, 3, 1)

        chat_container = st.container()
        with chat_container:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

        if prompt := st.chat_input("ãƒ•ãƒ©ãƒ³ã‚±ãƒ³AIã«èããŸã„ã“ã¨ã‚’å…¥åŠ›:"):
            if not st.session_state.conversation_id:
                st.session_state.conversation_id = str(uuid.uuid4())
            
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(prompt)

            turn_id = str(uuid.uuid4())
            st.session_state.request_id = turn_id
            
            log_extra_user = { 'json_fields': { 'conversation_id': st.session_state.conversation_id, 'request_id': turn_id, 'query': prompt } }
            logger.info("æ–°ã—ã„ã‚¯ã‚¨ãƒªã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚", extra=log_extra_user)

            with chat_container:
                with st.chat_message("assistant"):
                    message_placeholder = st.empty()
                    
                    response_obj = get_chat_response(
                        index=llama_index,
                        query=prompt,
                        chat_history=st.session_state.messages[:-1],
                        n_value=n_value,
                        system_prompt=custom_system_prompt,
                        context_prompt_template=CONTEXT_PROMPT_TEMPLATE,
                        condense_prompt_template=CONDENSE_QUESTION_PROMPT_TEMPLATE
                    )

                    if response_obj:
                        full_response_text = ""
                        reasoning_text = ""
                        
                        # [å¤‰æ›´] LLMã‹ã‚‰ã®å¿œç­”ã‚’JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
                        try:
                            # ```json ... ``` ã®ã‚ˆã†ãªãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰Šé™¤
                            raw_json = response_obj.response.strip().replace("```json", "").replace("```", "")
                            parsed_response = json.loads(raw_json)
                            full_response_text = parsed_response.get("response", "å›ç­”ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                            reasoning_text = parsed_response.get("reasoning", "ç†ç”±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        except (json.JSONDecodeError, AttributeError) as e:
                            logger.warning(f"LLMã®å¿œç­”ã®JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", extra={'json_fields': {'response_text': response_obj.response}})
                            full_response_text = str(response_obj.response) # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ãã®ã¾ã¾è¡¨ç¤º
                            reasoning_text = "JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—"

                        message_placeholder.markdown(full_response_text)
                        st.session_state.last_response_obj = response_obj
                        
                        source_nodes_for_log = [
                            {"text": node.text, "score": node.score} for node in response_obj.source_nodes
                        ]
                        chat_history_for_log = st.session_state.messages[:-1]

                        # [å¤‰æ›´] ãƒ­ã‚°ã«reasoningã‚’è¿½åŠ 
                        log_extra_assistant = {
                            'json_fields': {
                                'conversation_id': st.session_state.conversation_id,
                                'request_id': turn_id,
                                'query': prompt,
                                'response': full_response_text,
                                'reasoning': reasoning_text, # ç”Ÿæˆç†ç”±ã‚’è¨˜éŒ²
                                'source_nodes': source_nodes_for_log,
                                'chat_history': chat_history_for_log,
                            }
                        }
                        logger.info("LLMã‹ã‚‰ã®å›ç­”ã¨é–¢é€£æƒ…å ±ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚", extra=log_extra_assistant)
                        
                        st.session_state.messages.append({"role": "assistant", "content": full_response_text})
                    else:
                        error_message = "ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                        message_placeholder.error(error_message)
                        log_extra_error = { 'json_fields': { 'conversation_id': st.session_state.conversation_id, 'request_id': turn_id } }
                        logger.error("LLMã‹ã‚‰ã®å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", extra=log_extra_error)
                        st.session_state.messages.append({"role": "assistant", "content": error_message})

            st.session_state.feedback_submitted = False
            st.session_state.scroll_to_bottom = True
            st.rerun()

        if st.session_state.last_response_obj:
            with st.expander("å‚ç…§ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª"):
                for i, node in enumerate(st.session_state.last_response_obj.source_nodes):
                    st.markdown(f"--- **ã‚½ãƒ¼ã‚¹ {i+1} (é–¢é€£åº¦: {node.score:.4f})** ---")
                    st.text_area("", value=node.text, height=150, disabled=True, key=f"chunk_{i}")

            st.markdown("---")
            st.subheader("ğŸ“ ã“ã®å›ç­”ã«ã¤ã„ã¦ã®æ„Ÿæƒ³")

            if st.session_state.feedback_submitted:
                st.success("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼")
            else:
                with st.form(key='feedback_form'):
                    ratings = { "busso_doai": st.slider("1. ç‰©é¨’åº¦åˆã„", 1, 5, 3), "datousei": st.slider("2. å¦¥å½“æ€§", 1, 5, 3), "igaisei": st.slider("3. æ„å¤–æ€§", 1, 5, 3), "humor": st.slider("4. ãƒ¦ãƒ¼ãƒ¢ã‚¢", 1, 5, 3) }
                    feedback_comment = st.text_area("ãã®ä»–ã‚³ãƒ¡ãƒ³ãƒˆ:")
                    submit_button = st.form_submit_button(label='ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é€ä¿¡')

                    if submit_button:
                        last_user_message = st.session_state.messages[-2] if len(st.session_state.messages) > 1 else {}
                        last_assistant_message = st.session_state.messages[-1] if st.session_state.messages else {}
                        feedback_log_extra = { 'json_fields': { 'conversation_id': st.session_state.conversation_id, 'request_id': st.session_state.request_id, 'query': last_user_message.get('content', ''), 'response': last_assistant_message.get('content', ''), 'ratings': ratings, 'comment': feedback_comment } }
                        logger.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æ„Ÿæƒ³ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚", extra=feedback_log_extra)
                        st.session_state.feedback_submitted = True
                        st.session_state.last_response_obj = None
                        st.rerun()

    if st.session_state.get('scroll_to_bottom', False):
        components.html(
            """
            <script>
                window.scrollTo(0, document.body.scrollHeight);
            </script>
            """,
            height=0,
            width=0,
        )
        st.session_state.scroll_to_bottom = False

if __name__ == "__main__":
    main()
