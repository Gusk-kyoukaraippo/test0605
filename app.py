import os
import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext, load_index_from_storage, PromptTemplate
from llama_index.llms.google_genai import GoogleGenAI
# from llama_index.embeddings.google_genai import GoogleGenerativeAIEmbedding # ç¾åœ¨ã®LlamaIndexã§ã¯Settings.embed_modelã§æ˜ç¤ºçš„ã«è¨­å®šã—ãªãã¦ã‚‚è‰¯ã„å ´åˆãŒå¤šã„ã§ã™

# --- Gemini APIã‚­ãƒ¼ã®è¨­å®š ---
# Streamlit Secretsã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨
try:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'.streamlit/secrets.toml' ãƒ•ã‚¡ã‚¤ãƒ«ã« GOOGLE_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop() # APIã‚­ãƒ¼ãŒãªã„å ´åˆã¯ã‚¢ãƒ—ãƒªã®å®Ÿè¡Œã‚’åœæ­¢

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
INDEX_DIR = "storage"

# â˜…ã‚«ã‚¹ã‚¿ãƒ QAãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å®šç¾©ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
DEFAULT_QA_PROMPT = """
ã‚ãªãŸã¯ã€æä¾›ã•ã‚ŒãŸã€Œå‚ç…§æƒ…å ±ã€ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œè³ªå•ã€ã«æ˜ç¢ºã‹ã¤ç°¡æ½”ã«å›ç­”ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„:

1.  ã€Œå‚ç…§æƒ…å ±ã€ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œè³ªå•ã€ã«ã€Œæœ€çµ‚å›ç­”ã€ã‚’1ï½2æ–‡ã§ä½œã£ã¦ãã ã•ã„
2.  ã€Œæœ€çµ‚å›ç­”ã€ã¨ã€Œè³ªå•ã€ã‚’çµã¶èª¬æ˜ã‚’ã€Œå‚ç…§æƒ…å ±ã€ã‚’å‚è€ƒã«ã—ã¤ã¤ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã§ä½œæˆã—ã¦ãã ã•ã„

å‚ç…§æƒ…å ±:
---------------------
{context_str}
---------------------

è³ªå•:
{query_str}

å›ç­”:
"""

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ­ãƒ¼ãƒ‰ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦é«˜é€ŸåŒ–)
# @st.cache_resource ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã¯ã€é–¢æ•°ãŒåˆã‚ã¦å®Ÿè¡Œã•ã‚ŒãŸã¨ãã«çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€
# æ¬¡å›ä»¥é™ã®å®Ÿè¡Œã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçµæœã‚’å†åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã‚’çŸ­ç¸®ã—ã¾ã™ã€‚
@st.cache_resource
def load_llama_index(index_dir: str):
    """
    ä¿å­˜ã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
    """
    index_path = os.path.join(index_dir, "docstore.json")
    if not os.path.exists(index_path):
        st.error(f"ã‚¨ãƒ©ãƒ¼: '{index_dir}' ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.info("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é…ç½®ã—ã€å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ä¸€åº¦å®Ÿè¡Œã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
        return None
    
    st.spinner(f"'{index_dir}' ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­ã§ã™...")
    try:
        storage_context = StorageContext.from_defaults(persist_dir=index_dir)
        index = load_index_from_storage(storage_context)
        st.success("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚")
        return index
    except Exception as e:
        st.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.info("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å†ç”Ÿæˆã‚’è©¦ã¿ã¦ãã ã•ã„ã€‚")
        return None

def get_response_from_llm(index, query: str, n_value: int, custom_qa_template_str: str):
    """
    æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã‚¯ã‚¨ãƒªã€Nå€¤ã€ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦LLMã‹ã‚‰å¿œç­”ã‚’å–å¾—ã—ã¾ã™ã€‚
    """
    llm = GoogleGenAI(model="gemini-2.5-flash-preview-05-20") 
    qa_template = PromptTemplate(custom_qa_template_str)

    query_engine = index.as_query_engine(
        llm=llm,
        similarity_top_k=n_value,
        text_qa_template=qa_template
    ) 

    with st.spinner('AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­ã§ã™...'):
        response = query_engine.query(query)
    return response

# --- Streamlit UIã®æ§‹ç¯‰ ---
st.set_page_config(page_title="RAGãƒ™ãƒ¼ã‚¹QAã‚¦ã‚§ãƒ–ã‚¢ãƒ—ãƒª", layout="wide")
st.title("ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQAãƒœãƒƒãƒˆ")

st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã¯ã€æ—¢å­˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚
å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„é–¢é€£åº¦ (`N` å€¤) ã‚’èª¿æ•´ã—ã¦ã€AIã®å¿œç­”ã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚
---
""")

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ­ãƒ¼ãƒ‰
llama_index = load_llama_index(INDEX_DIR)

if llama_index:
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã®è¨­å®š
    st.sidebar.header("âš™ï¸ è¨­å®š")

    # Nå€¤ã®èª¿æ•´ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼
    n_value = st.sidebar.slider(
        "é¡ä¼¼åº¦ãƒˆãƒƒãƒ—K (Nå€¤)", 
        min_value=1, 
        max_value=20, 
        value=5, 
        step=1,
        help="å›ç­”ç”Ÿæˆã®ãŸã‚ã«å–å¾—ã™ã‚‹æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯ã®æ•°ã‚’è¨­å®šã—ã¾ã™ã€‚"
    )
    st.sidebar.info(f"ä¸Šä½ **{n_value}** å€‹ã®é–¢é€£æ€§ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®èª¿æ•´ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢
    st.sidebar.subheader("ğŸ“ ã‚«ã‚¹ã‚¿ãƒ QAãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    custom_prompt_text = st.sidebar.text_area(
        "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç·¨é›†ã—ã¦ãã ã•ã„ (context_str ã¨ query_str ã¯å¿…é ˆã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã§ã™)",
        DEFAULT_QA_PROMPT,
        height=400,
        help="AIã«æŒ‡ç¤ºã‚’ä¸ãˆã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¾ã™ã€‚`{context_str}` ã¨ `{query_str}` ã¯å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚"
    )

    st.sidebar.markdown("---")
    st.sidebar.write("Â© 2024 LlamaIndex Streamlit Demo")

    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢
    st.header("è³ªå•ã‚’ã—ã¦ãã ã•ã„")
    user_query = st.text_input("ã“ã“ã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", placeholder="ä¾‹: ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ä¸»è¦ãªã‚³ãƒ³ã‚»ãƒ—ãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ")

    if user_query:
        if "context_str" not in custom_prompt_text or "query_str" not in custom_prompt_text:
            st.warning("ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã¯ `{context_str}` ã¨ `{query_str}` ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            st.info(f"è³ªå•: **{user_query}**")
            response = get_response_from_llm(llama_index, user_query, n_value, custom_prompt_text)
            
            st.subheader("ğŸ¤– å›ç­”")
            st.write(str(response))

            # è©³ç´°æƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            with st.expander("è©³ç´°æƒ…å ±ã‚’è¦‹ã‚‹"):
                st.write("**ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:**")
                st.code(custom_prompt_text, language='text')
                st.write(f"**ä½¿ç”¨ã•ã‚ŒãŸNå€¤:** {n_value}")
                if hasattr(response, 'source_nodes'):
                    st.write("**å‚ç…§ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒ£ãƒ³ã‚¯:**")
                    for i, node in enumerate(response.source_nodes):
                        st.write(f"--- ãƒãƒ£ãƒ³ã‚¯ {i+1} ---")
                        st.text(node.text)
                        if node.metadata:
                            st.json(node.metadata)
                else:
                    st.info("ã‚½ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")

else:
    st.warning("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€è³ªå•ã«å›ç­”ã§ãã¾ã›ã‚“ã€‚ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")