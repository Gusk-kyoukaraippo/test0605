import os
import shutil
import streamlit as st
import json
import tempfile
from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    PromptTemplate,
    Settings,
)
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from google.cloud import storage

# --- ãƒšãƒ¼ã‚¸è¨­å®š (æœ€åˆã«ä¸€åº¦ã ã‘å‘¼ã³å‡ºã™) ---
st.set_page_config(page_title="RAGãƒ™ãƒ¼ã‚¹QAã‚¦ã‚§ãƒ–ã‚¢ãƒ—ãƒª (GCSå¯¾å¿œ)", layout="wide")

# --- å®šæ•°å®šç¾© ---
LOCAL_INDEX_DIR = "downloaded_storage"
DEFAULT_QA_PROMPT = """
ã‚ãªãŸã¯ã€æä¾›ã•ã‚ŒãŸã€Œå‚ç…§æƒ…å ±ã€ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œè³ªå•ã€ã«æ˜ç¢ºã‹ã¤ç°¡æ½”ã«å›ç­”ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„:

1.  ã€Œå‚ç…§æƒ…å ±ã€ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œè³ªå•ã€ã«ã€Œæœ€çµ‚å›ç­”ã€ã‚’1ï½2æ–‡ã§ä½œã£ã¦ãã ã•ã„
2.  ã€Œæœ€çµ‚å›ç­”ã€ã¨ã€Œè³ªå•ã€ã‚’çµã¶èª¬æ˜ã‚’ã€Œå‚ç…§æƒ…å ±ã€ã‚’å‚è€ƒã«ã—ã¤ã¤ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã§ä½œæˆã—ã¦ãã ã•ã„

å‚ç…§æƒ…å ±:
---------------------
{context_str}
---------------------

è³ªå•:
{query_str}

å›ç­”:
"""

# --- 1. APIã‚­ãƒ¼ã¨GCSèªè¨¼æƒ…å ±ã®è¨­å®š ---
temp_gcs_key_path = None
try:
    # Streamlit secretsã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
    GCS_BUCKET_NAME = st.secrets["GCS_BUCKET_NAME"]
    GCS_INDEX_PREFIX = st.secrets["GCS_INDEX_PREFIX"]
    gcs_service_account_json_str = st.secrets["GCS_SERVICE_ACCOUNT_JSON"]

    # GCSã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    try:
        parsed_json = json.loads(gcs_service_account_json_str)
        clean_json_str = json.dumps(parsed_json)
    except json.JSONDecodeError as e:
        st.error(f"Streamlit secretsã®'GCS_SERVICE_ACCOUNT_JSON'ãŒä¸æ­£ãªJSONå½¢å¼ã§ã™: {e}")
        st.info(
            "secrets.tomlã®GCPã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãŒæ­£ã—ã„JSONå½¢å¼ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            "ç‰¹ã«ã€ä¸‰é‡å¼•ç”¨ç¬¦(`\"\"\"`)ã§å›²ã‚€ã¨æ”¹è¡Œã‚„ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã®å•é¡ŒãŒèµ·ãã«ãããªã‚Šã¾ã™ã€‚"
        )
        st.stop()

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«èªè¨¼æƒ…å ±ã‚’æ›¸ãå‡ºã—ã€ç’°å¢ƒå¤‰æ•°ã«ãƒ‘ã‚¹ã‚’è¨­å®š
    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".json", encoding="utf-8") as tmp_file:
        tmp_file.write(clean_json_str)
        temp_gcs_key_path = tmp_file.name
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = temp_gcs_key_path

except KeyError as e:
    st.error(
        f"å¿…è¦ãªè¨­å®šãŒ secrets.toml ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}ã€‚"
        "GOOGLE_API_KEY, GCS_BUCKET_NAME, GCS_INDEX_PREFIX, GCS_SERVICE_ACCOUNT_JSON ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
    )
    st.stop()
except Exception as e:
    st.error(f"è¨­å®šã®èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    st.exception(e)
    st.stop()


# --- 2. LlamaIndexé–¢é€£ã®é–¢æ•° ---
@st.cache_resource
def load_llama_index_from_gcs():
    """
    Google Cloud Storage (GCS) ã‹ã‚‰LlamaIndexã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚
    """
    if os.path.exists(LOCAL_INDEX_DIR):
        shutil.rmtree(LOCAL_INDEX_DIR)
    os.makedirs(LOCAL_INDEX_DIR, exist_ok=True)

    st.info(f"GCSãƒã‚±ãƒƒãƒˆ '{GCS_BUCKET_NAME}' ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")

    try:
        client = storage.Client()
        bucket = client.bucket(GCS_BUCKET_NAME)
        blobs = list(bucket.list_blobs(prefix=GCS_INDEX_PREFIX))

        if not blobs or all(blob.name == GCS_INDEX_PREFIX and blob.size == 0 for blob in blobs):
            st.warning(f"GCSãƒã‚±ãƒƒãƒˆ '{GCS_BUCKET_NAME}' ã® '{GCS_INDEX_PREFIX}' ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None

        download_count = 0
        for blob in blobs:
            if blob.name == GCS_INDEX_PREFIX or blob.name.endswith('/'):
                continue
            relative_path = os.path.relpath(blob.name, GCS_INDEX_PREFIX)
            local_file_path = os.path.join(LOCAL_INDEX_DIR, relative_path)
            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
            blob.download_to_filename(local_file_path)
            download_count += 1

        if download_count == 0:
            st.warning(f"GCSã® '{GCS_INDEX_PREFIX}' ãƒ‘ã‚¹ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None

        st.success(f"{download_count} å€‹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")

        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
        embed_model = GoogleGenAIEmbedding(model_name="models/text-embedding-004")

        # ãƒ­ãƒ¼ã‚«ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰
        storage_context = StorageContext.from_defaults(persist_dir=LOCAL_INDEX_DIR)
        index = load_index_from_storage(storage_context)
        st.success("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚è³ªå•ã‚’å…¥åŠ›ã§ãã¾ã™ã€‚")
        return index

    except Exception as e:
        st.error(f"GCSã‹ã‚‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.exception(e)
        st.info(
            "ä»¥ä¸‹ã®ç‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:\n"
            f"- GCSãƒã‚±ãƒƒãƒˆå ('{GCS_BUCKET_NAME}') ã¨ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ ('{GCS_INDEX_PREFIX}') ãŒæ­£ã—ã„ã‹ã€‚\n"
            "- 'GCS_SERVICE_ACCOUNT_JSON' ãŒæ­£ã—ãã€é©åˆ‡ãªæ¨©é™ã‚’æŒã£ã¦ã„ã‚‹ã‹ã€‚\n"
            "- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãŒå®‰å®šã—ã¦ã„ã‚‹ã‹ã€‚"
        )
        return None

def get_response_from_llm(index: VectorStoreIndex, query: str, n_value: int, custom_qa_template_str: str):
    """
    LLMã‚’ä½¿ç”¨ã—ã¦ã€LlamaIndexã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    try:
        llm = GoogleGenAI(model="gemini-2.5-flash-preview-05-20")
        qa_template = PromptTemplate(custom_qa_template_str)
        query_engine = index.as_query_engine(
            llm=llm,
            similarity_top_k=n_value,
            text_qa_template=qa_template
        )
        with st.spinner('AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­ã§ã™...'):
            response = query_engine.query(query)
        return response
    except Exception as e:
        st.error(f"LLMã‹ã‚‰ã®å¿œç­”å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.exception(e)
        st.info("Gemini APIã‚­ãƒ¼ãŒæœ‰åŠ¹ã‹ã€é¸æŠã—ãŸãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None

# --- 3. Streamlit UIã®æ§‹ç¯‰ ---
def main():
    """
    Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³UIã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
    """
    st.title("ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQAãƒœãƒƒãƒˆ (GCSé€£æº)")
    st.markdown("""
    ã“ã®ã‚¢ãƒ—ãƒªã¯ã€GCSã«ä¿å­˜ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ã„ã€å†…å®¹ã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚
    å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã€æ¤œç´¢è¨­å®šã‚„AIã¸ã®æŒ‡ç¤ºï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã‚’èª¿æ•´ã§ãã¾ã™ã€‚
    """)
    st.markdown("---")

    llama_index = load_llama_index_from_gcs()

    if llama_index:
        st.sidebar.header("âš™ï¸ é«˜åº¦ãªè¨­å®š")
        n_value = st.sidebar.slider(
            "é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢æ•° (Nå€¤)", 1, 10, 3, 1,
            help="å›ç­”ç”Ÿæˆã®éš›ã«å‚ç…§ã™ã‚‹ã€é–¢é€£æ€§ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚"
        )
        st.sidebar.info(f"ç¾åœ¨ã€ä¸Šä½ **{n_value}** å€‹ã®é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")

        st.sidebar.subheader("ğŸ“ ã‚«ã‚¹ã‚¿ãƒ QAãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
        custom_prompt_text = st.sidebar.text_area(
            "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç·¨é›† ({context_str}ã¨{query_str}ã¯å¿…é ˆ):",
            DEFAULT_QA_PROMPT, height=350,
            help="AIã¸ã®æŒ‡ç¤ºã§ã™ã€‚`{context_str}`(å‚ç…§æƒ…å ±)ã¨`{query_str}`(è³ªå•)ã‚’å«ã‚ã¦ãã ã•ã„ã€‚"
        )
        st.sidebar.markdown("---")
        st.sidebar.caption("Â© 2024 RAG Demo")

        st.header("ğŸ’¬ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        user_query = st.text_input(
            "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«é–¢ã™ã‚‹è³ªå•ã‚’ã“ã“ã«å…¥åŠ›ã—ã¦ãã ã•ã„:",
            placeholder="ä¾‹: ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸»è¦ãªãƒ†ãƒ¼ãƒã¯ä½•ã§ã™ã‹ï¼Ÿ"
        )

        if user_query:
            if "{context_str}" not in custom_prompt_text or "{query_str}" not in custom_prompt_text:
                st.warning("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã¯`{context_str}`ã¨`{query_str}`ã®ä¸¡æ–¹ã‚’å«ã‚ã¦ãã ã•ã„ã€‚")
            else:
                response = get_response_from_llm(llama_index, user_query, n_value, custom_prompt_text)
                if response:
                    st.subheader("ğŸ¤– AIã‹ã‚‰ã®å›ç­”")
                    st.write(str(response))
                    
                    # å‚ç…§ã‚½ãƒ¼ã‚¹ã®è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ã§ã¯ãªãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®æƒ…å ±æä¾›ã¨ã—ã¦ï¼‰
                    if response.source_nodes:
                        with st.expander("å‚ç…§ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª"):
                            for i, node in enumerate(response.source_nodes):
                                st.markdown(f"--- **ã‚½ãƒ¼ã‚¹ {i+1} (é–¢é€£åº¦: {node.score:.2f})** ---")
                                st.text_area(
                                    label=f"ã‚½ãƒ¼ã‚¹ {i+1} ã®å†…å®¹",
                                    value=node.text, 
                                    height=150, 
                                    disabled=True,
                                    key=f"chunk_{i}"
                                )


    else:
        st.error(
            "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãŸã‚ã€QAã‚·ã‚¹ãƒ†ãƒ ã‚’èµ·å‹•ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            "ãƒšãƒ¼ã‚¸ä¸Šéƒ¨ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèªã—ã€è¨­å®šã‚„GCSã®çŠ¶æ…‹ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚"
        )

if __name__ == "__main__":
    main()
